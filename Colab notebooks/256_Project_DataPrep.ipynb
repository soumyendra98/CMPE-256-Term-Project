{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSB5bfPefJFx",
        "outputId": "b3f25573-f8ea-4e98-d981-2a9568ae2fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ],
      "source": [
        "# Imports and settings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-kDa1fUfXxa",
        "outputId": "152595ff-87d1-4ec2-e814-c1894fe72dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Import"
      ],
      "metadata": {
        "id": "1C_62mgvmfvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "\n",
        "reviews = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/256_data/RAW_interactions.csv')\n",
        "recipes = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/256_data/RAW_recipes.csv')"
      ],
      "metadata": {
        "id": "t-jQdAlNfYEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine recipes and reviews into one df\n",
        "food = reviews.merge(recipes, left_on = 'recipe_id', right_on = 'id')"
      ],
      "metadata": {
        "id": "WScEHZ4zf1y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "MgdPDic-mjBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting string column of nutrition values into separate float values columns\n",
        "\n",
        "food[['calories','total fat','sugar','sodium','protein','saturated fat','carbohydrates']] = food.nutrition.str.split(',', expand = True) \n",
        "food['calories'] = food['calories'].apply(lambda x: x.replace('[' ,''))\n",
        "food['carbohydrates'] = food['carbohydrates'].apply(lambda x: x.replace(']' ,''))\n",
        "food[['calories','total fat','sugar','sodium','protein','saturated fat','carbohydrates']] =  food[['calories','total fat','sugar','sodium','protein','saturated fat','carbohydrates']].astype(float)"
      ],
      "metadata": {
        "id": "RfI-Ru4b7OHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop records where recipe has one review and it is by the the author of the recipe\n",
        "\n",
        "df_g_by_recipeid = food.groupby(['recipe_id']).agg({'user_id' : 'nunique', 'contributor_id' : 'nunique'}).reset_index()\n",
        "df_ = df_g_by_recipeid[df_g_by_recipeid.user_id == 1]\n",
        "idx_review_by_author = list(df_[df_.user_id == df_.contributor_id].index)\n",
        "food = food[~food.index.isin(idx_review_by_author)]"
      ],
      "metadata": {
        "id": "XUzr8IV9kJFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text columns from list to string\n",
        "\n",
        "def convert_to_list(data):\n",
        "  a = data.replace('-', ' ').replace('[', '').replace(']', '')\n",
        "  a = a.translate(str.maketrans('', '', string.punctuation))\n",
        "  return a\n",
        "\n",
        "food['tags'] = food['tags'].apply(lambda x: convert_to_list(x))\n",
        "food['ingredients'] = food['ingredients'].apply(lambda x: convert_to_list(x))\n",
        "food['steps'] = food['steps'].apply(lambda x: convert_to_list(x) )"
      ],
      "metadata": {
        "id": "xxKuV6z_mzmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns\n",
        "food.drop(['id', 'nutrition', 'date', 'submitted'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "jA-J-FqnnA02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns\n",
        "food.rename(columns = {'user_id':'reviewer_id', 'date':'review_date', 'submitted':'recipe_upload_date', 'total fat':'total_fat', 'saturated fat':'saturated_fat'}, inplace = True)"
      ],
      "metadata": {
        "id": "qA_GymDanD5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove outliers using IQR method\n",
        "\n",
        "def removeOutliers(column, food):\n",
        "  q25, q75 = np.percentile(food[column], 25), np.percentile(food[column], 75)\n",
        "  iqr = q75 - q25\n",
        "  cut_off = iqr * 1.5\n",
        "  lower, upper = q25 - cut_off, q75 + cut_off\n",
        "  food = food[(food.minutes >= lower) & (food.minutes <= upper)]\n",
        "\n",
        "removeOutliers('minutes', food)\n",
        "removeOutliers('n_steps', food)\n",
        "removeOutliers('calories', food)"
      ],
      "metadata": {
        "id": "eVVr4EafnHGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcCUBk5OnfzV",
        "outputId": "fb600ab8-4bf9-49a3-d778-e80f9a89ac46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1040414, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert datatype of text columns from object to string\n",
        "\n",
        "food['ingredients'] = food['ingredients'].astype('string')\n",
        "food['tags'] = food['tags'].astype('string')\n",
        "food['steps'] = food['steps'].astype('string')"
      ],
      "metadata": {
        "id": "KxEqR7S6nnkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing ratings with median rating\n",
        "\n",
        "food['rating_imputed'] = food['rating'].replace(0, np.nan)\n",
        "food['rating'] = food['rating_imputed'].fillna(food.groupby(['recipe_id'])['rating'].transform('median'))\n",
        "food.drop(['rating_imputed'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "Kr1sMnzawrkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "WGF2ZWWrzqOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subset of data with unique recipes only \n",
        "\n",
        "food_sample = food.drop_duplicates(subset = 'name', keep = 'first')\n",
        "food_sample = food_sample.reset_index(drop = True)"
      ],
      "metadata": {
        "id": "vHDVuysox4-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# derive type of recipe from ingredients\n",
        "non_veg = ['chicken', 'eggs', 'egg', 'beef', 'turkey', 'shrimp', 'scallop', 'breast', 'breasts', 'boneless', 'pork', 'ham', 'whites', 'fish', 'steaks', 'steak', 'yolks', 'tuna', 'meat', 'yolk' ,\\\n",
        "           'pultry', 'meatballs', 'catfish', 'eggnog', 'mincemeat', 'crawfish', 'goldfish', 'swordfish', 'wing', 'meatloaf', 'pigeon', 'octopus', 'quail', 'rabbit', 'liver', 'livers', \\\n",
        "           'liverwurst', 'whitefish', 'squid', 'lobster', 'oyster', 'oysters', 'deer', 'ribs', 'clam', 'clams', 'lamb', 'sheep', 'seafood', 'bacon', 'thighs', 'legs', 'poultry']\n",
        "\n",
        "food_sample['veg'] = 0\n",
        "food_sample['non_veg'] = 0\n",
        "\n",
        "# derive cuisine and course type from tags\n",
        "cuisines = ['american', 'european', 'asian', 'australian', 'indian', 'chinese', 'middle_eastern']\n",
        "food_sample['cuisine'] = ''\n",
        "\n",
        "dessert = ['desserts', 'dairy', 'cookies', 'brownies', 'cakes', 'chocolate', 'pies', 'muffins', 'pancakes', 'waffles',' puddings', 'mousses', 'smoothies', 'cream', 'ice cream', \\\n",
        "            'cheesecake', 'frostings', 'cake', 'cupcakes', 'shakes', 'jams', 'pastry', 'baking', 'biscotti', 'jellies', 'pie', 'sugar']\n",
        "food_sample['sweet'] = 0\n",
        "food_sample['savory'] = 0"
      ],
      "metadata": {
        "id": "q5yQc8DYyAb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get above features from existing features\n",
        "\n",
        "for index, row in food_sample.iterrows():\n",
        "  non_veg_flag = False\n",
        "  for j in non_veg:\n",
        "    if j in row.loc['ingredients']:\n",
        "      food_sample._set_value(index, 'non_veg', 1)\n",
        "      non_veg_flag = True\n",
        "      break\n",
        "  if not non_veg_flag:\n",
        "    food_sample._set_value(index, 'veg', 1)\n",
        "\n",
        "\n",
        "  dessert_flag = False\n",
        "  for j in dessert:\n",
        "    if j in row.loc['tags']:\n",
        "      food_sample._set_value(index, 'sweet', 1)\n",
        "      dessert_flag = True\n",
        "      break\n",
        "  if not dessert_flag:\n",
        "    food_sample._set_value(index, 'savory', 1)\n",
        "\n",
        "  \n",
        "  for j in cuisines:\n",
        "    if j in row.loc['tags']:\n",
        "      food_sample._set_value(index, 'cuisine', j)\n",
        "      break"
      ],
      "metadata": {
        "id": "aiMBV9SIyYb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean cuisine and replace empty with 'other'\n",
        "food_sample['cuisine'] = food_sample['cuisine'].replace(r'^\\s*$', 'other', regex=True)\n",
        "\n",
        "# one hot encode cuisine column\n",
        "food_sample = pd.get_dummies(food_sample, columns = ['cuisine'])"
      ],
      "metadata": {
        "id": "fY_9Rk8wybyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decide whether the food is healthy based on the total % and calories. If food has more than 2500 calories or has more than 30% of the daily reccommended intake of sugar, it is unhealthy.\n",
        "# However if there are 2 or more other factors that are more than 30% (60% for protein) than the daily recommended intake then it is deemed healthy\n",
        "\n",
        "def healthyTag(row):\n",
        "    count = 0\n",
        "    if float(row['calories']) >2500:\n",
        "        return 'healthy'\n",
        "    if float(row['total_fat']) > 30:\n",
        "        count += 1\n",
        "    if float(row['sugar']) > 30:\n",
        "        count += 2\n",
        "    if float(row['sodium']) > 30:\n",
        "        count += 1\n",
        "    if float(row['protein']) > 70:\n",
        "        count += 1\n",
        "    if float(row['saturated_fat']) > 30:\n",
        "        count += 1\n",
        "    if float(row['carbohydrates']) > 30:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        return 'unhealthy'\n",
        "    else:\n",
        "        return 'healthy'\n",
        "    \n",
        "# Create the label column, Healthy\n",
        "food_sample['healthy'] = food_sample.apply(lambda x: healthyTag(x), axis = 1)\n",
        "food_sample = pd.get_dummies(food_sample, columns = ['healthy'])\n",
        "\n",
        "food_sample.rename(columns={'healthy_healthy':'healthy', 'healthy_unhealthy':'unhealthy'}, inplace=True)"
      ],
      "metadata": {
        "id": "V6xrKqHVy6B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge food_sample with food\n",
        "\n",
        "food_encoded = pd.merge(food, food_sample[['recipe_id', 'veg', 'non_veg', 'sweet', 'savory', 'cuisine_american', 'cuisine_asian', 'cuisine_australian', \\\n",
        "                                                   'cuisine_european', 'cuisine_indian', 'cuisine_other']], how = 'left', on = 'recipe_id')"
      ],
      "metadata": {
        "id": "UryonKD7zVXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP processing\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "MIN_CHAR = 3\n",
        "MAX_CHAR = 150\n",
        "\n",
        "PATTERN_S = re.compile(\"\\'s\")    \n",
        "PATTERN_RN = re.compile(\"\\\\r\\\\n\") \n",
        "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\") \n",
        "\n",
        "# Remove spaces, new lin charcaters and punctuations\n",
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(PATTERN_S, '', text)\n",
        "  text = re.sub(PATTERN_RN, '', text)\n",
        "  text = re.sub(PATTERN_PUNC, '', text)\n",
        "  \n",
        "  return text\n",
        "\n",
        "# Tokenize text\n",
        "def tokenizer(text, lemmatize):\n",
        "\n",
        "  if lemmatize:\n",
        "    lemma = WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(w) for w in word_tokenize(text)]\n",
        "  else:\n",
        "    tokens = [w for w in word_tokenize(text)]\n",
        "  \n",
        "  tokens_ = [w for w in tokens if (len(w) >= MIN_CHAR and len(w) < MAX_CHAR and w not in STOPWORDS)]\n",
        "  \n",
        "  return tokens_\n",
        "\n",
        "# Wrapper to perform text cleaning activities\n",
        "def data_prep(col_str, df):\n",
        "\n",
        "  print('Cleaning text')\n",
        "  df['clean_' + col_str] = df[col_str].apply(clean_text)\n",
        "\n",
        "  print('Tokenization and Lemmatization in progress')\n",
        "  df['token_' + col_str] = df['clean_' + col_str].apply(lambda x : tokenizer(x, lemmatize = True))\n",
        "\n",
        "  df['rec_' + col_str] = df['token_' + col_str].apply(lambda x: ' '.join(map(str, x)))\n",
        "  \n",
        "  df.drop(['clean_' + col_str, 'token_' + col_str], inplace = True, axis = 1)\n",
        "  df = df.reset_index(drop = True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "n7aanTb9zbL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food_sample = data_prep('ingredients', food_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vB5HHI-2ymv",
        "outputId": "5ae36514-71c4-4605-c74b-a453e5fa202d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning text\n",
            "Tokenization and Lemmatization in progress\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "food_sample['ingredients_steps'] = food_sample['ingredients'] + ' ' + food_sample['steps']\n",
        "food_sample = data_prep('ingredients_steps', food_sample)\n",
        "food_sample.drop(['ingredients_steps'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRaswBdV3Fab",
        "outputId": "1d6b88cb-091d-4c84-b38d-1cdd66a0871e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning text\n",
            "Tokenization and Lemmatization in progress\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting combined text - cuisine, meal type, course type, healthiness, ingredients and tags\n",
        "\n",
        "temp = food_sample[['name', 'tags', 'ingredients', 'veg', 'non_veg', 'sweet', 'savory', 'cuisine_american', 'cuisine_asian', 'cuisine_australian', 'cuisine_european', \\\n",
        "                      'cuisine_indian', 'cuisine_other', 'healthy', 'unhealthy']]\n",
        "\n",
        "# Reverse one hot encoding\n",
        "temp['type'] = pd.get_dummies(temp[['veg', 'non_veg']]).idxmax(1)\n",
        "temp['meal'] = pd.get_dummies(temp[['sweet', 'savory']]).idxmax(1)\n",
        "temp['cuisine'] = pd.get_dummies(temp[['cuisine_american', 'cuisine_asian', 'cuisine_australian', 'cuisine_european', 'cuisine_indian', 'cuisine_other']]).idxmax(1)\n",
        "temp['healthy_unhealthy'] = pd.get_dummies(temp[['healthy', 'unhealthy']]).idxmax(1)\n",
        "\n",
        "# Drop one hot encoded columns\n",
        "temp.drop(['veg', 'non_veg', 'sweet', 'savory', 'cuisine_american', 'cuisine_asian', 'cuisine_australian', 'cuisine_european', 'cuisine_indian', 'cuisine_other', 'healthy', 'unhealthy'],\n",
        "                    axis = 1, inplace = True)\n",
        "\n",
        "# Replace values\n",
        "temp['cuisine'] = temp['cuisine'].replace({'cuisine_american':'american', 'cuisine_asian':'asian', 'cuisine_australian':'australian', \n",
        "                                   'cuisine_european':'european', 'cuisine_indian':'indian', 'cuisine_other':'other'})\n",
        "\n",
        "# Create a combined string\n",
        "temp['combined'] = temp['cuisine'] + ' ' + temp['healthy_unhealthy'] + ' ' + temp['type'] + ' ' + temp['meal'] + ' made of ' + temp['ingredients'] + \\\n",
        "' tagged under ' + temp['tags']\n",
        "\n",
        "# Drop extra columns and remove nans\n",
        "temp = temp.drop(['tags', 'ingredients', 'type', 'meal', 'cuisine', 'healthy_unhealthy'], axis = 1)\n",
        "temp.dropna(subset = ['combined'], inplace = True)\n",
        "\n",
        "temp.reset_index(drop = True, inplace = True)\n",
        "\n",
        "# Merge combined comlumn with food_sample and rename \n",
        "food_sample = pd.merge(food_sample, temp, how = 'left', on = 'name')\n",
        "food_sample.rename(columns = {'combined':'rec_combined'}, inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rwqTLIN3LOS",
        "outputId": "5699a512-1b59-4e79-ec40-bd420753bb25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-21f6a37e4332>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  temp['type'] = pd.get_dummies(temp[['veg', 'non_veg']]).idxmax(1)\n",
            "<ipython-input-30-21f6a37e4332>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  temp['meal'] = pd.get_dummies(temp[['sweet', 'savory']]).idxmax(1)\n",
            "<ipython-input-30-21f6a37e4332>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  temp['cuisine'] = pd.get_dummies(temp[['cuisine_american', 'cuisine_asian', 'cuisine_australian', 'cuisine_european', 'cuisine_indian', 'cuisine_other']]).idxmax(1)\n",
            "<ipython-input-30-21f6a37e4332>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  temp['healthy_unhealthy'] = pd.get_dummies(temp[['healthy', 'unhealthy']]).idxmax(1)\n",
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n",
            "<ipython-input-30-21f6a37e4332>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  temp['cuisine'] = temp['cuisine'].replace({'cuisine_american':'american', 'cuisine_asian':'asian', 'cuisine_australian':'australian',\n",
            "<ipython-input-30-21f6a37e4332>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  temp['combined'] = temp['cuisine'] + ' ' + temp['healthy_unhealthy'] + ' ' + temp['type'] + ' ' + temp['meal'] + ' made of ' + temp['ingredients'] + \\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write out dataframes\n",
        "\n",
        "food_sample.to_csv('/content/drive/MyDrive/Colab Notebooks/256_data/food_sample.csv')\n",
        "food_encoded.to_csv('/content/drive/MyDrive/Colab Notebooks/256_data/food.csv')"
      ],
      "metadata": {
        "id": "T7cVSfDv4OC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GFpGeu3Q4k_q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}